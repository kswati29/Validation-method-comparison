{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Different Validation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By:\n",
    "#### Swati Kohli & Poonam Patil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Supervised Machine Learning using Lasso Regression with different Validation Methods\n",
    "#### The notebook is an analytics exercise exploring the use of Lasso regression in scikit-learn as an efficient tool   for a high dimensional dataset (too many predictors, too few observations), as a first step is to screen out insignificant features.  However, the goal is to demonstrate different validation methods used to avoid overfitting and their comparison along with pros and cons of each towards model building.\n",
    "### Objective:\n",
    "#### Implement 3 validation methods by building a Lasso regression model to predict the total number of non-violate crimes (per 100k population) and compare performance. \n",
    "Original Dataset Source: https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "### Technique:  \n",
    "#### The validation method is used to determine best model. The three validation methods used for computation here are:\n",
    "1. Train / Validation / Test split\n",
    "2. 5-Fold Cross Validation\n",
    "3. 10-Fold Cross Validation \n",
    "  \n",
    "#### For Lasso Regression, hyperparameter selection is done for the following parameters:\n",
    "1. **alpha:** the lamba value in the penalty\n",
    "2. **max iter:** the maximum number of iterations for optimization algorithm (Min. 50).\n",
    "3. **tol:** the tolerance for optimization (max 0.1)  \n",
    "\n",
    "### Performance Comparison\n",
    "#### Finally, the results are compared across the three methods along the following metrics :\n",
    "1. Time taken for hyperparameter selection\n",
    "2. Coefficients of selected predictors\n",
    "3. Prediction results or MSE on test set.\n",
    "\n",
    "### Pre processing\n",
    "The dataset was cleaned beforehand to work with and finally consists of 2118 observations, and 101 predictors + 1 response (total number of non-violent crimes).  \n",
    "All relevant libraries are imported for the task, dataset loaded and classified as X (Predictor variables) and y(Response Variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant Libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# For Lasso Regression\n",
    "from sklearn import linear_model # For LASSO Regression \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics # For evaluation\n",
    "from sklearn.metrics import mean_squared_error # For evaluation\n",
    "from sklearn.preprocessing import StandardScaler # For scaling/standardizing dataset\n",
    "\n",
    "# For Validation methods\n",
    "from sklearn.model_selection import train_test_split # Dataset Splitting\n",
    "import itertools # To form all Hyperparameter combination pairs\n",
    "from sklearn.pipeline import Pipeline # Package to perform instructions \n",
    "from sklearn.model_selection import GridSearchCV # CV method\n",
    "\n",
    "import time # For time evaluation\n",
    "\n",
    "import warnings # Suppress warnings because they are annoying\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>nonViolPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11980</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>91.78</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.88</td>\n",
       "      <td>12.47</td>\n",
       "      <td>21.44</td>\n",
       "      <td>10.93</td>\n",
       "      <td>11.33</td>\n",
       "      <td>...</td>\n",
       "      <td>10.66</td>\n",
       "      <td>53.72</td>\n",
       "      <td>65.29</td>\n",
       "      <td>78.09</td>\n",
       "      <td>89.14</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1845.9</td>\n",
       "      <td>9.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1394.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23123</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>95.57</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11.01</td>\n",
       "      <td>21.30</td>\n",
       "      <td>10.48</td>\n",
       "      <td>17.18</td>\n",
       "      <td>...</td>\n",
       "      <td>8.30</td>\n",
       "      <td>77.17</td>\n",
       "      <td>71.27</td>\n",
       "      <td>90.22</td>\n",
       "      <td>96.12</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2186.7</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29344</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>94.33</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.35</td>\n",
       "      <td>11.36</td>\n",
       "      <td>25.88</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.28</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>44.77</td>\n",
       "      <td>36.60</td>\n",
       "      <td>61.26</td>\n",
       "      <td>82.85</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2780.9</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6167.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11245</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.53</td>\n",
       "      <td>89.16</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.52</td>\n",
       "      <td>24.46</td>\n",
       "      <td>40.53</td>\n",
       "      <td>28.69</td>\n",
       "      <td>12.65</td>\n",
       "      <td>...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>73.75</td>\n",
       "      <td>42.22</td>\n",
       "      <td>60.34</td>\n",
       "      <td>89.02</td>\n",
       "      <td>11.5</td>\n",
       "      <td>974.2</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9988.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140494</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>95.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.09</td>\n",
       "      <td>32.89</td>\n",
       "      <td>20.04</td>\n",
       "      <td>13.26</td>\n",
       "      <td>...</td>\n",
       "      <td>1.49</td>\n",
       "      <td>64.35</td>\n",
       "      <td>42.29</td>\n",
       "      <td>70.61</td>\n",
       "      <td>85.66</td>\n",
       "      <td>70.4</td>\n",
       "      <td>1995.7</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0       11980           3.10          1.37         91.78          6.50   \n",
       "1       23123           2.82          0.80         95.57          3.44   \n",
       "2       29344           2.43          0.74         94.33          3.43   \n",
       "3       11245           2.76          0.53         89.16          1.17   \n",
       "4      140494           2.45          2.51         95.65          0.90   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  ...  \\\n",
       "0         1.88        12.47        21.44        10.93       11.33  ...   \n",
       "1         0.85        11.01        21.30        10.48       17.18  ...   \n",
       "2         2.35        11.36        25.88        11.01       10.28  ...   \n",
       "3         0.52        24.46        40.53        28.69       12.65  ...   \n",
       "4         0.95        18.09        32.89        20.04       13.26  ...   \n",
       "\n",
       "   PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
       "0           10.66             53.72           65.29          78.09   \n",
       "1            8.30             77.17           71.27          90.22   \n",
       "2            5.00             44.77           36.60          61.26   \n",
       "3            1.74             73.75           42.22          60.34   \n",
       "4            1.49             64.35           42.29          70.61   \n",
       "\n",
       "   PctSameState85  LandArea  PopDens  PctUsePubTrans  LemasPctOfficDrugUn  \\\n",
       "0           89.14       6.5   1845.9            9.63                  0.0   \n",
       "1           96.12      10.6   2186.7            3.84                  0.0   \n",
       "2           82.85      10.6   2780.9            4.37                  0.0   \n",
       "3           89.02      11.5    974.2            0.38                  0.0   \n",
       "4           85.66      70.4   1995.7            0.97                  0.0   \n",
       "\n",
       "   nonViolPerPop  \n",
       "0        1394.59  \n",
       "1        1955.95  \n",
       "2        6167.51  \n",
       "3        9988.79  \n",
       "4        6867.42  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "community = pd.read_csv('community.csv')\n",
    "community.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify as X & y (Predictors and Response variable)\n",
    "X = community.copy()\n",
    "del X['nonViolPerPop']\n",
    "y = community['nonViolPerPop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 1** - Train, Validation & Test set  \n",
    "**About**  \n",
    "In Train & Test method approach, there is quite a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. Therefore, to avoid data leakage, instead of train test, Train, Validation & Test set method is used where dataset is partitioned in 3 parts. The training is done on the training set, post that evaluation is done on the validation set, and ultimately, final evaluation is done on the test set.\n",
    "For this exercise, 50%, 20% & 30% dataset split is used.\n",
    "\n",
    "**Approach:**  \n",
    "Dataset is split\n",
    "1. Split the dataset and create Train, Validation & test set of 50-21-30% respectively.\n",
    "2. Set up the combination of various candidate values for hyperparameter selection.\n",
    "2. Scale the training data and transform training and Valid set. This is to change the values of numeric columns in the dataset to a common scale.\n",
    "3. Perform Lasso to learn the best hyperparameters based on MSE of validation set and compute time taken for this process.\n",
    "4. Scale train + valid and transform test set\n",
    "5. Fit the train + valid set with best lambda\n",
    "6. Predict and find MSE on test set\n",
    "7. Extract the final features, coefficients and their count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ******** lasso method 1 : Train / validation / test split method ******** \n",
    "\n",
    "# 1. Split the dataset and create Train, Validation & test set of 50-21-30% respectively.\n",
    "\n",
    "# Create 70% - 30 % split as train-test split of original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 861)\n",
    "\n",
    "# create 70% - 30% as training-validation split of train set (30% of train set is 21% of original data set)\n",
    "X_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size = 0.3, random_state = 861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set up candidate values for hyper parameter selection\n",
    "lambdas= np.logspace(-10, 10, 21) # tuning parameter \n",
    "max_iter = [50,60,70]             # Maximum number of iterations taken for the solver to converge\n",
    "tol= [0.0001, 0.001, 0.01, 0.1]   # Tolerance for stopping criteria\n",
    "\n",
    "# Now we will form sets of lambdas, max_iter and tol using the itertools library to form all combination pairs.\n",
    "hyperparameter_sets = list(itertools.product(lambdas, max_iter, tol)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter selection execution time in seconds: 2.747811794281006\n",
      "Min. validation MSE : 2361000.874450426\n",
      "Best hyper parameter set : (10.0, 70, 0.0001)\n",
      "MSE of train-valid-test split method on test set: 3590601.4151061825\n"
     ]
    }
   ],
   "source": [
    "# 3. Scale the data\n",
    "\n",
    "start1 = time.time() # record start time\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_training)\n",
    "X_training = pd.DataFrame(scaler.transform(X_training))\n",
    "X_valid = pd.DataFrame(scaler.transform(X_valid))\n",
    "\n",
    "\n",
    "# 4. Lasso to evaluate best hyperparameter\n",
    "validation_mse =[] \n",
    "\n",
    "for ind, sets in enumerate(hyperparameter_sets):\n",
    "    lm= Lasso(alpha =sets[0], max_iter=sets[1],tol=sets[2])\n",
    "    lm.fit(X_training, y_training)         # fit lasso on training set\n",
    "    # predict on validation set\n",
    "    validation_mse.append(metrics.mean_squared_error(lm.predict(X_valid), y_valid))\n",
    "\n",
    "end1 = time.time() # record end time\n",
    "\n",
    "print('Hyperparameter selection execution time in seconds:',end1-start1)\n",
    "print('Min. validation MSE :', min(validation_mse))\n",
    "print('Best hyper parameter set :',hyperparameter_sets[np.argmin(validation_mse)])\n",
    "\n",
    "# select best hyper parameters\n",
    "bestlambda = hyperparameter_sets[np.argmin(validation_mse)][0]\n",
    "best_max_iter= hyperparameter_sets[np.argmin(validation_mse)][1]\n",
    "best_tol= hyperparameter_sets[np.argmin(validation_mse)][2]\n",
    "\n",
    "\n",
    "# 5. Now fit scaler on train set and then transform train and test set for standardization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "X_train.columns = X.columns.values # to assign column names to X_train set as Scaler transform resets column names\n",
    "\n",
    "\n",
    "# 6. fit lasso on train set using best hyper parameters selected\n",
    "lm=Lasso(alpha = bestlambda, max_iter = best_max_iter, tol=best_tol)\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 7. Evaluate model on test set\n",
    "MSE_method1 = metrics.mean_squared_error(lm.predict(X_test),y_test)\n",
    "print('MSE of train-valid-test split method on test set:',MSE_method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant variables in Lasso method 1: 69\n"
     ]
    }
   ],
   "source": [
    "# 8. Extract the final features, coefficients and their count. \n",
    "\n",
    "# save lasso method1 coefficient in a dataframe\n",
    "lasso_method1 = pd.DataFrame(zip(X_train.columns.values,lm.coef_))\n",
    "\n",
    "# rename column names\n",
    "lasso_method1.columns = ['Predictor', 'Lasso_Coef']\n",
    "\n",
    "# set index to predictor\n",
    "lasso_method1.set_index('Predictor',inplace=True)\n",
    "\n",
    "# select lasso method1 significant predictor variables\n",
    "lasso_signi_vars = lasso_method1[lasso_method1['Lasso_Coef'] != 0]\n",
    "print('Number of significant variables in Lasso method 1:',lasso_signi_vars.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**  \n",
    "Lasso method 1 selected 69 predictor variables which are significant for modeling. \n",
    "We will compare the model performance later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 2** - 5 fold CV + test set  \n",
    "**About**\n",
    "Cross Validation method with k fold is used for Hyperparameter tuning and better learning. A test set is still held out for final evaluation. The method performs CV where the training set is split into k smaller sets(in this case 5), training is done on training sets which is k-1 folds and then validation is done on remaining data to compute the performance measure(MSE) which is averaged over the loops of each hyperparameter set. Further refinement is done for further tuning the best hyperparamters. Final evaluation is done on the test set.  \n",
    "\n",
    "**Approach:**  \n",
    "1. Divide train and test set. (Train/Test partition of method 1 used with same hold out set).\n",
    "2. Setup Pipeline to  \n",
    "    a. Scale the data  \n",
    "    b. Lasso algorithm for hyperparamter selection\n",
    "3. Set parameters for each item in the pipeline\n",
    "4. Perform CV 5 fold through a sparse Grid Search to find best hyperparameters.\n",
    "5. Refine the grid search further for tuning the best hyperparameter and rerun the process.\n",
    "6. Predict and find MSE on test set with refined hyperparameters.\n",
    "7. Extract the final features, coefficients and their count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter selection execution time in seconds: 17.3836829662323\n",
      "Best hyper parameter set : {'lasso__alpha': 10.0, 'lasso__max_iter': 50, 'lasso__tol': 0.0001}\n",
      "MSE of CV5 on test set: 3589793.910006581\n"
     ]
    }
   ],
   "source": [
    "#   ******** Method-2 : lasso regression with 5 fold cross validation ******** \n",
    "\n",
    "# 2. Set up the model pipeline\n",
    "start2_1 = time.time() # record start time\n",
    "\n",
    "estimator = Pipeline(steps = [('scale', StandardScaler()), # Scale the data\n",
    "                     ('lasso', Lasso()) ]) # fit the scaled data using Lasso\n",
    "\n",
    "# 3. Set up the parameters for each item in pipeline\n",
    "parameters = {'lasso__alpha': np.logspace(-10,10,21),'lasso__max_iter': [50,60,70],\n",
    "              'lasso__tol': [0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "# 4. Instantiate gridsearch cross validation for the model in pipeline\n",
    "reg2_1 = GridSearchCV(estimator = estimator, param_grid = parameters, cv = 5, \n",
    "                   scoring = 'neg_mean_squared_error', n_jobs = -1) # Instantiate the gridsearch\n",
    "\n",
    "# fit the model on train data\n",
    "reg2_1.fit(X_train, y_train)\n",
    "\n",
    "end2_1 = time.time() # record end time\n",
    "print('Hyper parameter selection execution time in seconds:',end2_1-start2_1)\n",
    "print('Best hyper parameter set :',reg2_1.best_params_)   # The best parameter from CV\n",
    "\n",
    "print('MSE of CV5 on test set:',mean_squared_error(reg2_1.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 fold CV Parameter Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Hyperparameter selection execution time in seconds: 31.418373823165894\n",
      "Refined hyperparameter set : {'lasso__alpha': 19.0, 'lasso__max_iter': 80, 'lasso__tol': 0.01}\n",
      "MSE of refined CV5 on test set: 3696196.0844516433\n"
     ]
    }
   ],
   "source": [
    "# 5. Best Hyperparameter set refinement\n",
    "\n",
    "start2_2 = time.time() # record start time\n",
    "\n",
    "# Set up the refined parameters\n",
    "parameters = {'lasso__alpha': np.linspace(1,20,20),'lasso__max_iter': [60,65,70,75,80],\n",
    "              'lasso__tol': [0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "# Instantiate gridsearch cross validation\n",
    "reg2_2 = GridSearchCV(estimator = estimator, param_grid = parameters, cv = 5, \n",
    "                   scoring = 'neg_mean_squared_error', n_jobs = -1) \n",
    "\n",
    "# Fit the grid search, i.e. perform CV and grid search. \n",
    "reg2_2.fit(X_train, y_train) \n",
    "\n",
    "end2_2 = time.time() # record end time\n",
    "\n",
    "print('Refined Hyperparameter selection execution time in seconds:', end2_2-start2_2)\n",
    "print('Refined hyperparameter set :', reg2_2.best_params_) # The best parameter from CV\n",
    "\n",
    "MSE_method2 = mean_squared_error(reg2_2.predict(X_test), y_test)\n",
    "print('MSE of refined CV5 on test set:',MSE_method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant variables in Lasso method-2: 56\n"
     ]
    }
   ],
   "source": [
    "# 6. Extract the final features, coefficients and their count. \n",
    "lasso_method2 = reg2_2.best_estimator_.named_steps['lasso'].coef_\n",
    "lasso2 = pd.DataFrame(zip(X_train.columns.values,lasso_method2))\n",
    "lasso2.columns = ['Predictor', 'LassoCV5_Coef']\n",
    "\n",
    "# set index to predictor\n",
    "lasso2.set_index('Predictor',inplace=True)\n",
    "\n",
    "# select lasso method2 significant predictor variables\n",
    "lasso_signi_vars2 =lasso2[lasso2['LassoCV5_Coef'] != 0]\n",
    "print('Number of significant variables in Lasso method-2:', lasso_signi_vars2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**  \n",
    "Lasso method 2 selected 56 predictor variables which are significant for modeling. \n",
    "We will compare the model performance later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method 3** - 10 fold CV + test set  \n",
    "CV 10 fold is same as CV 5 fold in terms of approach. The only difference is the k folds will be 10 in this case. Also,Train/Test partition of method 1 used with same hold out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter selection execution time in seconds: 32.18776512145996\n",
      "Best hyper parameter pair : {'lasso__alpha': 10.0, 'lasso__max_iter': 50, 'lasso__tol': 0.0001}\n",
      "MSE of CV10 on test set: 3589793.910006581\n"
     ]
    }
   ],
   "source": [
    "#   ******** Method-3 : lasso regression with 10 fold cross validation ********\n",
    "\n",
    "# 2. Set up the model pipeline\n",
    "start3_1 = time.time() # record start time\n",
    "\n",
    "estimator = Pipeline(steps = [('scale', StandardScaler()), # Scale the data\n",
    "                     ('lasso', Lasso()) ]) # regression model to use\n",
    "\n",
    "# 3. Set up the parameters for each item in pipeline\n",
    "parameters = {'lasso__alpha': np.logspace(-10,10,21),'lasso__max_iter': [50,60,70],'lasso__tol': [0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "# 4. Instantiate gridsearch cross validation for the model in pipeline\n",
    "reg3_1 = GridSearchCV(estimator = estimator, param_grid = parameters, cv = 10, \n",
    "                   scoring = 'neg_mean_squared_error', n_jobs = -1) \n",
    "\n",
    "# Fit the grid search, i.e. perform CV and grid search. \n",
    "reg3_1.fit(X_train, y_train) \n",
    "\n",
    "end3_1 = time.time() # record end time\n",
    "\n",
    "print('Hyper parameter selection execution time in seconds:',end3_1-start3_1)\n",
    "print('Best hyper parameter pair :', reg3_1.best_params_) # The best parameter from CV\n",
    "print('MSE of CV10 on test set:',mean_squared_error(reg3_1.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 fold CV Parameter Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Hyperparameter selection execution time in seconds: 55.72141122817993\n",
      "Best hyper parameter pair : {'lasso__alpha': 13.0, 'lasso__max_iter': 60, 'lasso__tol': 0.1}\n",
      "MSE of refined CV10 on test set: 3589793.910006581\n"
     ]
    }
   ],
   "source": [
    "# 5. Best Hyperparameter set refinement\n",
    "\n",
    "start3_2 = time.time() # record start time\n",
    "\n",
    "# Set up the refined parameters\n",
    "parameters = {'lasso__alpha': np.linspace(1,20,20),'lasso__max_iter': [60,65,70,75,80],'lasso__tol': [0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "# Instantiate gridsearch cross validation\n",
    "reg3_2 = GridSearchCV(estimator = estimator, param_grid = parameters, cv = 10, \n",
    "                   scoring = 'neg_mean_squared_error', n_jobs = -1) \n",
    "\n",
    "# Fit the grid search, i.e. perform CV and grid search. \n",
    "reg3_2.fit(X_train, y_train) \n",
    "\n",
    "end3_2 = time.time()\n",
    "print('Refined Hyperparameter selection execution time in seconds:',end3_2-start3_2) # set end time\n",
    "print('Best hyper parameter pair :', reg3_2.best_params_) # The best parameter from CV\n",
    "\n",
    "MSE_method3 = mean_squared_error(reg3_1.predict(X_test), y_test)\n",
    "print('MSE of refined CV10 on test set:',MSE_method3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant variables in Lasso method-3: 66\n"
     ]
    }
   ],
   "source": [
    "# 6. Extract the final features, coefficients and their count. \n",
    "\n",
    "# Extract regression coefficients of model\n",
    "lasso_method3 = reg3_2.best_estimator_.named_steps['lasso'].coef_\n",
    "lasso3=pd.DataFrame(zip(X_train.columns.values,lasso_method3))\n",
    "lasso3.columns = ['Predictor', 'LassoCV10_Coef']\n",
    "lasso3.set_index('Predictor',inplace=True)\n",
    "\n",
    "# select lasso method3 significant predictor variables\n",
    "lasso_signi_vars3 =lasso3[lasso3['LassoCV10_Coef'] != 0]\n",
    "print('Number of significant variables in Lasso method-3:',lasso_signi_vars3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**  \n",
    "Lasso method 3 selected 66 predictor variables which are significant for modeling. \n",
    "## Lets compare the model performances now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Evaluation\n",
    "Lets compare the performances across all the three methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 1 - Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter selection execution time in seconds:\n",
      "Time for train valid and test Method: 2.747811794281006\n",
      "Time for 5 fold CV Method: 31.418373823165894\n",
      "Time for 10 fold CV Method: 55.72141122817993\n"
     ]
    }
   ],
   "source": [
    "print('Hyperparameter selection execution time in seconds:')\n",
    "print('Time for train valid and test Method:',end1-start1)\n",
    "print('Time for 5 fold CV Method:',end2_2-start2_2)\n",
    "print('Time for 10 fold CV Method:',end3_2-start3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "Computational time for 10 fold CV is highest and that for train-valid-split method is lowest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 2 - Prediction Error comparison\n",
    "\n",
    "The comparison is demonstrated on test set for all the three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test set: 3590601.4151061825\n",
      "MSE of refined CV5 on test set: 3696196.0844516433\n",
      "MSE of refined CV10 on test set: 3589793.910006581\n"
     ]
    }
   ],
   "source": [
    "print('MSE on test set:',MSE_method1)\n",
    "print('MSE of refined CV5 on test set:',MSE_method2)\n",
    "print('MSE of refined CV10 on test set:',MSE_method3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "The comparison shows the least prediction error (MSE) for CV 10 fold method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 3 - Comparison of predictor coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lasso_Coef</th>\n",
       "      <th>LassoCV5_Coef</th>\n",
       "      <th>LassoCV10_Coef</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predictor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>householdsize</th>\n",
       "      <td>-152.807789</td>\n",
       "      <td>-107.140099</td>\n",
       "      <td>-217.709680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racepctblack</th>\n",
       "      <td>89.451908</td>\n",
       "      <td>118.645739</td>\n",
       "      <td>114.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racePctWhite</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racePctAsian</th>\n",
       "      <td>20.556429</td>\n",
       "      <td>3.855345</td>\n",
       "      <td>19.458797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Lasso_Coef  LassoCV5_Coef  LassoCV10_Coef\n",
       "Predictor                                               \n",
       "population       0.000000       0.000000        0.000000\n",
       "householdsize -152.807789    -107.140099     -217.709680\n",
       "racepctblack    89.451908     118.645739      114.344700\n",
       "racePctWhite    -0.000000      -0.000000       -0.000000\n",
       "racePctAsian    20.556429       3.855345       19.458797"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The coefficients extracted by three methods are put together for comparision of coefficients values.\n",
    "Coef_compare = pd.concat([lasso_method1, lasso2, lasso3], axis=1,join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features selected by Lasso method 1 are 69, 56 by method2 and 66 by method 3. Thus Lasso method 2 (i.e. 5 fold cross validation) minimizes many coeffients to zero thus proving best in feature selection.\n",
    "\n",
    "Also Lasso method 2 does maximum coefficient shrinkage for most of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lasso_Coef</th>\n",
       "      <th>LassoCV5_Coef</th>\n",
       "      <th>LassoCV10_Coef</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predictor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PctKids2Par</th>\n",
       "      <td>-822.267260</td>\n",
       "      <td>-897.761317</td>\n",
       "      <td>-409.737149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWSocSec</th>\n",
       "      <td>529.006152</td>\n",
       "      <td>375.839726</td>\n",
       "      <td>419.204422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MalePctDivorce</th>\n",
       "      <td>394.396462</td>\n",
       "      <td>367.172051</td>\n",
       "      <td>501.053654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PopDens</th>\n",
       "      <td>-378.781304</td>\n",
       "      <td>-311.951733</td>\n",
       "      <td>-337.557425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctPopUnderPov</th>\n",
       "      <td>361.630330</td>\n",
       "      <td>266.154889</td>\n",
       "      <td>477.237506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctEmploy</th>\n",
       "      <td>486.053597</td>\n",
       "      <td>264.377057</td>\n",
       "      <td>379.029790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctBornSameState</th>\n",
       "      <td>-250.985026</td>\n",
       "      <td>-254.108229</td>\n",
       "      <td>-268.803805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pctWRetire</th>\n",
       "      <td>-267.383353</td>\n",
       "      <td>-247.884818</td>\n",
       "      <td>-251.506784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <td>492.777624</td>\n",
       "      <td>205.096989</td>\n",
       "      <td>266.218797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnOccMedVal</th>\n",
       "      <td>-202.427118</td>\n",
       "      <td>-197.410750</td>\n",
       "      <td>-115.462639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Lasso_Coef  LassoCV5_Coef  LassoCV10_Coef\n",
       "Predictor                                                  \n",
       "PctKids2Par      -822.267260    -897.761317     -409.737149\n",
       "pctWSocSec        529.006152     375.839726      419.204422\n",
       "MalePctDivorce    394.396462     367.172051      501.053654\n",
       "PopDens          -378.781304    -311.951733     -337.557425\n",
       "PctPopUnderPov    361.630330     266.154889      477.237506\n",
       "PctEmploy         486.053597     264.377057      379.029790\n",
       "PctBornSameState -250.985026    -254.108229     -268.803805\n",
       "pctWRetire       -267.383353    -247.884818     -251.506784\n",
       "PctForeignBorn    492.777624     205.096989      266.218797\n",
       "OwnOccMedVal     -202.427118    -197.410750     -115.462639"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the best predictors in the model, we need to sort coefficients in descending order on its absolute value\n",
    "\n",
    "# create new column with absolute values of coefficients from method-2 as we selected method 2 as best model\n",
    "Coef_compare['abs_CV5_Coeff'] = abs(Coef_compare['LassoCV5_Coef'])\n",
    "\n",
    "# sort on new column with absolute coefficient values\n",
    "Coef_compare_sorted = Coef_compare.sort_values(by = 'abs_CV5_Coeff', ascending = False)\n",
    "\n",
    "# delete column with absolute values\n",
    "del Coef_compare_sorted['abs_CV5_Coeff']\n",
    "\n",
    "Coef_compare_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso 5-fold cross validation method selected PctKids2Par (percentage of kids in family housing with two parents) as best predictor of total number of non-violent crimes.\n",
    "\n",
    "#### Coefficint Interpretation for first two predictors with method2:\n",
    "1. If standerdize percentage of kids in family housing with two parents increases by 1 standard deviation then standerdize total number of non-violent crimes decreases by 897.76 standard deviation.\n",
    "2. If standerdize percentage of households with social security income in 1989 increases by 1 standard deviation then standerdize total number of non-violent crimes increases by 375.83 standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison Interpretation\n",
    "Since the datasize is small and number of predictors are large, in terms of feature selection, CV method (in particular 5 fold) is the best for initial model building and feature selection. This is because even though Train Valid Test method is quick, it's model fitting is predominatly based on chance(by applying hyperparameter selection on only one validation set split). This is also why it takes less time for computation and not very relaible for providing least error on hold out set(test set).\n",
    "\n",
    "Comparing between 5 Fold & 10 Fold execution time is less for 5 Fold. The CV 10 fold model performs best in error prediction as MSE is reduced by almost 3% from CV 5 to CV 10. But there is a trade off between prediction result and computational time (18 seconds vs almost 1 minute!) & power for hyperparameter selection. For initial model building even though, 10 fold CV provides the best model fitting out of the three methods, since it takes higher computational power and time, it is not a preffered method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of the Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1** \n",
    "\n",
    "PROS  :Least computational time so is a fast method\n",
    "\n",
    "CONS  :However, by partitioning the data into three sets, the number of samples are reduced significantly which could have been used for learning the model, the results depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "**Method 2 & 3**  \n",
    "Pros  : There is less data wastage, less dependence on luck for fixing an arbitrary validation set. Therefore better model fitting.\n",
    "\n",
    "Cons  : Computationally expensive.\n",
    "\n",
    "**Method 2 vs method 3**  \n",
    "Longer computational time and power in 10 fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Due to the option of training on multiple train-test split (as k folds), cross validation is the preffered method predominantly. It also gives a probability of better performance on unseen data(hold-out set). Train Valid Test method, on the other hand, only trains on one validation set which makes the result dependent on chance or luck.\n",
    "Method-1 is good for initial model building or with a very large dataset because it is much quicker compared to CV method that takes more computational power and time to run.\n",
    "The CV method is a more accurate representation of how the model will perform on unseen data than Train Valid Test method.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
